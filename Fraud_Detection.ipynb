{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP70B5m2XgMsciTNuo87Tde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joel-Mk/Fraud-Detection/blob/main/Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, precision_recall_curve,\n",
        "    roc_curve, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, OneClassSVM\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "pFkATDjeN_NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "\n",
        "def load_creditcard_csv(path):\n",
        "    \"\"\"\n",
        "    Expects Kaggle Credit Card Fraud (2013) format with columns:\n",
        "    V1..V28 (numeric PCA comps), 'Time', 'Amount', 'Class' (0/1).\n",
        "    Returns X (DataFrame), y (Series).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    # Basic sanity\n",
        "    assert 'Class' in df.columns, \"CSV must contain 'Class' as target\"\n",
        "    y = df['Class'].astype(int)\n",
        "    X = df.drop(columns=['Class'])\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "_WhsDCFZONW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_synthetic(n=100_000, fraud_rate=0.005, random_state=42):\n",
        "    \"\"\"\n",
        "    Create a synthetic fraud dataset with numeric & simple categorical features.\n",
        "    Highly imbalanced; signals baked in with subtle shifts for fraud class.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Numerical features\n",
        "    amount = np.abs(rng.normal(50, 30, n))  # skew-ish amounts\n",
        "    time_of_day = rng.integers(0, 24, n)    # hours\n",
        "    days_since_signup = rng.exponential(90, n)\n",
        "\n",
        "    # Categoricals\n",
        "    country = rng.choice(['US', 'IN', 'GB', 'DE', 'BR', 'ZA', 'SG'], size=n, p=[0.35,0.25,0.1,0.1,0.1,0.05,0.05])\n",
        "    channel = rng.choice(['web', 'app', 'ivr'], size=n, p=[0.6, 0.35, 0.05])\n",
        "    device = rng.choice(['ios', 'android', 'desktop'], size=n, p=[0.35, 0.45, 0.2])\n",
        "\n",
        "    # Latent fraud indicator\n",
        "    y = (rng.random(n) < fraud_rate).astype(int)\n",
        "\n",
        "    # Inject signal:\n",
        "    # Fraud tends to be higher during odd hours, higher amounts, certain channels/countries\n",
        "    amount += y * rng.normal(80, 50, n)          # larger amounts when fraud\n",
        "    odd_hour = ((time_of_day < 4) | (time_of_day > 22)).astype(int)\n",
        "    risky_country = np.isin(country, ['BR', 'SG']).astype(int)\n",
        "    risky_channel = (channel == 'web').astype(int)\n",
        "    risky_device = (device == 'android').astype(int)\n",
        "\n",
        "    # Generate a few engineered features (you can pretend these came from domain knowledge)\n",
        "    amount_per_day = amount / (1 + days_since_signup)\n",
        "    risk_score = 0.02*amount + 0.6*odd_hour + 0.7*risky_country + 0.3*risky_channel + 0.3*risky_device \\\n",
        "                 + rng.normal(0, 0.3, n)\n",
        "    # Increase probability for fraud class\n",
        "    y = ((risk_score > np.quantile(risk_score, 0.995)) | (y == 1)).astype(int)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'amount': amount,\n",
        "        'time_of_day': time_of_day,\n",
        "        'days_since_signup': days_since_signup,\n",
        "        'country': country,\n",
        "        'channel': channel,\n",
        "        'device': device,\n",
        "        'amount_per_day': amount_per_day,\n",
        "        'risk_hint': risk_score\n",
        "    })\n",
        "    return df, pd.Series(y, name='Class')\n"
      ],
      "metadata": {
        "id": "kdESb3OKOT_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_splits(X, y, test_size=0.2, stratified=True, random_state=42):\n",
        "\n",
        "    if stratified:\n",
        "        return train_test_split(\n",
        "            X, y, test_size=test_size, stratify=y, random_state=random_state\n",
        "        )\n",
        "    else:\n",
        "        # fall back to simple split without stratification\n",
        "        return train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=False)"
      ],
      "metadata": {
        "id": "Feptf2u5Ochx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_preprocessor(X):\n",
        "\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "    # We'll implement a lightweight preprocessor with two steps inside a Pipeline:\n",
        "    # 1) pandas.get_dummies for categoricals\n",
        "    # 2) StandardScaler for all numeric columns (after dummies)\n",
        "    class PandasDummiesScaler:\n",
        "        def __init__(self, num_cols, cat_cols):\n",
        "            self.num_cols = num_cols\n",
        "            self.cat_cols = cat_cols\n",
        "            self.columns_ = None\n",
        "            self.scaler_ = StandardScaler(with_mean=False)  # CSR-friendly if needed\n",
        "\n",
        "        def fit(self, X, y=None):\n",
        "            X_ = pd.get_dummies(X, columns=self.cat_cols, drop_first=True)\n",
        "            self.columns_ = X_.columns\n",
        "            self.scaler_.fit(X_[self.num_cols])\n",
        "            return self\n",
        "\n",
        "        def transform(self, X):\n",
        "            X_ = pd.get_dummies(X, columns=self.cat_cols, drop_first=True)\n",
        "            # align columns (handle unseen categories)\n",
        "            for col in self.columns_:\n",
        "                if col not in X_.columns:\n",
        "                    X_[col] = 0\n",
        "            X_ = X_[self.columns_]\n",
        "            # scale numeric subset\n",
        "            X_[self.num_cols] = self.scaler_.transform(X_[self.num_cols])\n",
        "            return X_.values\n",
        "\n",
        "    return PandasDummiesScaler(num_cols, cat_cols)"
      ],
      "metadata": {
        "id": "2j3gDsorOgVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_logistic(C=1.0, class_weight=\"balanced\", max_iter=2000):\n",
        "    return LogisticRegression(\n",
        "        C=C,\n",
        "        class_weight=class_weight,\n",
        "        max_iter=max_iter,\n",
        "        n_jobs=None,\n",
        "        solver=\"liblinear\",  # robust for small/medium data and class_weight\n",
        "    )\n",
        "\n",
        "def build_linear_svm(C=1.0, class_weight=\"balanced\"):\n",
        "    # Need probability calibration to get PR/ROC; wrap with CalibratedClassifierCV\n",
        "    base = LinearSVC(C=C, class_weight=class_weight)\n",
        "    model = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
        "    return model\n",
        "\n",
        "def build_oneclass_svm(nu=0.01, gamma='scale'):\n",
        "    # Unsupervised anomaly detector; we score then threshold by top K% as fraud\n",
        "    return OneClassSVM(nu=nu, gamma=gamma)\n"
      ],
      "metadata": {
        "id": "X6uHrYx8Oqpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_smote(X, y, use_smote=True, random_state=42):\n",
        "    if use_smote and IMBLEARN_AVAILABLE:\n",
        "        sm = SMOTE(random_state=random_state, sampling_strategy=0.1)  # oversample minority to 10%\n",
        "        X_res, y_res = sm.fit_resample(X, y)\n",
        "        return X_res, y_res\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "mCnvC-HaO6Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_predict(model, preprocessor, X_train, y_train, X_valid):\n",
        "    preprocessor.fit(X_train, y_train)\n",
        "    Xtr = preprocessor.transform(X_train)\n",
        "    Xva = preprocessor.transform(X_valid)\n",
        "    model.fit(Xtr, y_train)\n",
        "    # predict_proba if available; else decision_function\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        p_valid = model.predict_proba(Xva)[:, 1]\n",
        "    else:\n",
        "        # CalibratedClassifierCV provides predict_proba; for OneClassSVM we'll handle separately\n",
        "        if hasattr(model, \"decision_function\"):\n",
        "            scores = model.decision_function(Xva)\n",
        "            # Scale to 0-1 via min-max for comparability\n",
        "            p_valid = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
        "        else:\n",
        "            # fallback\n",
        "            p_valid = model.predict(Xva)\n",
        "    return p_valid"
      ],
      "metadata": {
        "id": "0IgsyGwbO_jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_score, threshold=None, label=\"Model\"):\n",
        "    roc = roc_auc_score(y_true, y_score)\n",
        "    pr_auc = average_precision_score(y_true, y_score)\n",
        "    print(f\"== {label} ==\")\n",
        "    print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
        "\n",
        "    if threshold is None:\n",
        "        # Choose threshold that maximizes F2 on validation\n",
        "        precision, recall, thr = precision_recall_curve(y_true, y_score)\n",
        "        beta = 2.0\n",
        "        f2 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + 1e-12)\n",
        "        best_idx = np.nanargmax(f2)\n",
        "        threshold = thr[max(best_idx - 1, 0)] if best_idx < len(thr) else 0.5\n",
        "        print(f\"Chosen threshold (F2-optimal): {threshold:.4f}\")\n",
        "\n",
        "    y_pred = (y_score >= threshold).astype(int)\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    return threshold\n"
      ],
      "metadata": {
        "id": "IB1pzBqzPEOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curves(y_true, y_score, title_suffix=\"\"):\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(fpr, tpr, label=\"ROC\")\n",
        "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"ROC Curve {title_suffix}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # PR\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(recall, precision, label=\"PR\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"Precision-Recall Curve {title_suffix}\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Z5xk2x5-PJ4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(\n",
        "    data_source=\"synthetic\",\n",
        "    data_path=None,\n",
        "    test_size=0.2,\n",
        "    use_smote=False,\n",
        "    model_name=\"logreg\",\n",
        "    random_state=42\n",
        "):\n",
        "    \"\"\"\n",
        "    data_source: \"csv\" or \"synthetic\"\n",
        "    model_name: \"logreg\" | \"linsvm\" | \"oneclass\"\n",
        "    \"\"\"\n",
        "    # 1) Load data\n",
        "    if data_source == \"csv\":\n",
        "        assert data_path and os.path.exists(data_path), \"Provide valid data_path for CSV mode\"\n",
        "        X, y = load_creditcard_csv(data_path)\n",
        "    else:\n",
        "        X, y = make_synthetic(n=120_000, fraud_rate=0.006, random_state=random_state)\n",
        "\n",
        "    # 2) Split\n",
        "    X_train, X_valid, y_train, y_valid = make_splits(X, y, test_size=test_size, stratified=True, random_state=random_state)\n",
        "\n",
        "    # 3) Preprocessor\n",
        "    pre = build_preprocessor(X)\n",
        "\n",
        "    # 4) Choose model\n",
        "    if model_name == \"logreg\":\n",
        "        model = build_logistic(C=1.0, class_weight=\"balanced\")\n",
        "    elif model_name == \"linsvm\":\n",
        "        model = build_linear_svm(C=1.0, class_weight=\"balanced\")\n",
        "    elif model_name == \"oneclass\":\n",
        "        # One-class uses only \"normal\" class for training\n",
        "        normal_mask = (y_train == 0)\n",
        "        model = build_oneclass_svm(nu=0.01)\n",
        "        # Fit on normal only\n",
        "        pre.fit(X_train[normal_mask], y_train[normal_mask])\n",
        "        Xtr = pre.transform(X_train[normal_mask])\n",
        "        model.fit(Xtr)\n",
        "        # Scoring on valid\n",
        "        Xva = pre.transform(X_valid)\n",
        "        scores = -model.decision_function(Xva)  # higher = more anomalous\n",
        "        y_score = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
        "        print(\"== One-Class SVM (unsupervised baseline) ==\")\n",
        "        threshold = evaluate(y_valid, y_score, threshold=None, label=\"OneClassSVM\")\n",
        "        plot_curves(y_valid, y_score, title_suffix=\"(OneClassSVM)\")\n",
        "        return {\n",
        "            \"model\": model, \"preprocessor\": pre, \"threshold\": threshold\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model_name\")\n",
        "\n",
        "    # 5) Optional: SMOTE on training only (supervised models)\n",
        "    if use_smote:\n",
        "        if IMBLEARN_AVAILABLE:\n",
        "            Xtr_df = pd.DataFrame(pre.fit_transform(X_train, y_train))\n",
        "            Xtr_df.columns = [f\"f{i}\" for i in range(Xtr_df.shape[1])]\n",
        "            X_res, y_res = maybe_smote(Xtr_df, y_train, use_smote=True, random_state=random_state)\n",
        "            model.fit(X_res, y_res)\n",
        "            # Score on valid\n",
        "            y_score = fit_predict(model, pre, X_train, y_train, X_valid)\n",
        "        else:\n",
        "            print(\"imblearn not available; proceeding without SMOTE.\")\n",
        "            y_score = fit_predict(model, pre, X_train, y_train, X_valid)\n",
        "    else:\n",
        "        y_score = fit_predict(model, pre, X_train, y_train, X_valid)\n",
        "\n",
        "    # 6) Evaluate + plots\n",
        "    label = \"LogisticRegression\" if model_name == \"logreg\" else \"LinearSVM (calibrated)\"\n",
        "    threshold = evaluate(y_valid, y_score, threshold=None, label=label)\n",
        "    plot_curves(y_valid, y_score, title_suffix=f\"({label})\")\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"preprocessor\": pre,\n",
        "        \"threshold\": threshold\n",
        "    }"
      ],
      "metadata": {
        "id": "UsZmeDmXOzuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba_pipeline(pipeline_dict, X_new_df):\n",
        "    \"\"\"\n",
        "    Given trained {model, preprocessor, threshold},\n",
        "    return probabilities and binary predictions.\n",
        "    \"\"\"\n",
        "    model = pipeline_dict[\"model\"]\n",
        "    pre = pipeline_dict[\"preprocessor\"]\n",
        "    thr = pipeline_dict[\"threshold\"]\n",
        "\n",
        "    Xn = pre.transform(X_new_df)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        p = model.predict_proba(Xn)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        scores = model.decision_function(Xn)\n",
        "        p = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
        "    else:\n",
        "        p = model.predict(Xn)\n",
        "\n",
        "    pred = (p >= thr).astype(int)\n",
        "    return p, pred\n"
      ],
      "metadata": {
        "id": "0wlczyZ3PNnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Option A: CSV (Kaggle creditcard.csv)\n",
        "    # PIPE = run_pipeline(data_source=\"csv\", data_path=\"creditcard.csv\", model_name=\"logreg\", use_smote=False)\n",
        "\n",
        "    # Option B: Synthetic (works out of the box)\n",
        "    PIPE = run_pipeline(data_source=\"synthetic\", model_name=\"logreg\", use_smote=False)\n",
        "\n",
        "    # Example: inference on 5 new samples (synthetic)\n",
        "    X_new, _ = make_synthetic(n=5, fraud_rate=0.2, random_state=7)  # higher fraud-rate for demo\n",
        "    probs, preds = predict_proba_pipeline(PIPE, X_new)\n",
        "    print(\"\\nSample inference:\")\n",
        "    print(pd.DataFrame({\"prob_fraud\": probs, \"pred\": preds}))"
      ],
      "metadata": {
        "id": "QTI2B1fzPRQd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}